\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}

% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Red box around equation
\usepackage[skins, theorems]{tcolorbox}
\tcbset{highlight math style={enhanced, colframe=red,colback=white,arc=0pt,boxrule=1pt}}

% Code listing with highlight
\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{style1}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=lines,
    language=Python,
    columns=fullflexible,
}

\lstset{style=style1}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{16824 Homework 3}
\author{Jiajun Wan \\ Andrew ID: jiajunw2}
\date{}

\begin{document}
\maketitle

\setcounter{section}{1}
\section*{Task 1: Understanding VQA}

\setcounter{subsection}{0}
\subsection{Why do you think that it's computationally costly to use all answers as separate classes instead of keeping only the most frequent ones?}
We have to use a large number of output neurons, that add computations on final sigmoid/softmax calculation, loss computation, and backward propagation. Keeping only the most frequent ones will largely reduce the amount of calculations.

\subsection{Complete the \_\_init\_\_ method of the dataset class for VQA in vqa\_dataset.py. Specifically, initialize the VQA API and anything you need from that.}
\begin{lstlisting}
self._vqa = VQA(annotation_file=annotation_json_file_path,
                question_file=question_json_file_path)
\end{lstlisting}

\subsection{Implement the \_\_len\_\_ method of the VQADataset class. Should the size of the dataset be equal to the number of images, questions or the answers?}
\begin{lstlisting}
def __len__(self):
    # total number of questions
    return len(self._vqa.qqa)
\end{lstlisting}
The size of the dataset be equal to the number of questions.

\subsection{Complete the \_\_getitem\_\_ method}
\begin{lstlisting}
q_anno = self._vqa.qa[idx]  # load annotation
q_str = self._vqa.qqa[idx]['question']  # question in str format
\end{lstlisting}
\newpage

\setcounter{section}{2}
\section*{Task 2: Building a pipeline for VQA}

\setcounter{subsection}{0}
\subsection{What should be the output dimension of the trainable linear layer? Complete the corresponding part in the \_\_init\_\_ method of BaselineNet in models.py.}
\begin{lstlisting}
self.classifier = nn.Linear(
    self.text_encoder.config.hidden_size + 512,
    n_answers,
)
\end{lstlisting}
It should be n\_answers (5127)

\subsection{Implement compute\_vis\_feats that featurizes the image (it can be implemented as an one-liner!).}
\begin{lstlisting}
def compute_vis_feats(self, image):
    """Convert image tensors to feature tensors."""
    return torch.nn.functional.adaptive_avg_pool2d(self.vis_encoder(image), 1).squeeze() # feed to vis_encoder and then mean pool on spatial dims
\end{lstlisting}

\subsection{Implement the forward pass of BaselineNet. Make sure to use compute\_vis\_feats and compute\_text\_feats.}
\begin{lstlisting}
def forward(self, image, question):
    """Forward pass, image (B, 3, 224, 224), qs list of str."""
    text_feats = self.compute_text_feats(question)
    vis_feats = self.compute_vis_feats(image)
    concate_feats = torch.hstack((text_feats, vis_feats))
    logits = self.classifier(concate_feats)
    return logits
\end{lstlisting}

\subsection{What is the loss for multi-label classification? (Hint: in HW1 you also tackled a multi-class classification problem)}
Binary Cross-Entropy Loss

\subsection{Implement the loss call and the optimization code in the train\_test\_loop method.}
\begin{lstlisting}
loss = F.binary_cross_entropy_with_logits(scores, answers)

# Update
if mode == 'train':
    # optimize loss
    loss.backward()
    self.optimizer.step()
    self.optimizer.zero_grad()
\end{lstlisting}

\subsection{Complete the train\_test\_loop method to monitor the performance in Tensorboard. Plot the loss and accuracy and include these in your report. Additionally show multiple image-question pairs (at least 3) with the respective answers (predicted and ground-truth).}
\begin{lstlisting}
    # add code to show the question
    self.writer.add_text(
        'Question%d' % i, data['question'][i],
        epoch * _n_show + i
    )
    # the gt answer
    self.writer.add_text(
        'GT Answer%d' % i, self._id2answer[(data['answers'][i] == 1).nonzero(as_tuple=True)[0][0].item()],
        epoch * _n_show + i
    )
    # and the predicted answer
    self.writer.add_text(
        'Predicted Answer%d' % i, self._id2answer[scores.argmax(1)[i].item()],
        epoch * _n_show + i
    )
# add code to plot the current accuracy
self.writer.add_scalar(
'Acc/' + mode, n_correct / n_samples,
epoch * len(self.data_loaders[mode]) + step
)
\end{lstlisting}
\includegraphics[width=\textwidth]{q2.6_1.png}
\includegraphics[width=\textwidth]{q2.6_2.png}
\includegraphics[width=\textwidth]{q2.6_3.png}
\includegraphics[width=\textwidth]{q2.6_4.png}
Final Val Acc: 0.5341694647442228 \\

\begin{tabular}{ccc}
    \includegraphics[width=0.33\textwidth]{q2.6_5.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_6.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_7.png} \\
    \includegraphics[width=0.3\textwidth]{q2.6_8.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_9.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_10.png} \\
\end{tabular}
\begin{tabular}{ccc}
    \includegraphics[width=0.33\textwidth]{q2.6_11.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_12.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_13.png} \\
    \includegraphics[width=0.3\textwidth]{q2.6_14.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_15.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_16.png} \\
\end{tabular}
\begin{tabular}{ccc}
    \includegraphics[width=0.33\textwidth]{q2.6_17.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_18.png} &
    \includegraphics[width=0.33\textwidth]{q2.6_19.png} \\
    \includegraphics[width=0.3\textwidth]{q2.6_20.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_21.png} &
    \includegraphics[width=0.3\textwidth]{q2.6_22.png} \\
\end{tabular}

\subsection{Make a histogram of the frequencies of the predicted answers (all 5217) without labels.}
Show only the label with predictions \\
\includegraphics[width=\textwidth]{q2.7_1.png}
Show all the 5217 labels \\
\includegraphics[width=\textwidth]{q2.7_2.png}

\subsection{What are the 10 most frequently predicted classes? Why do you think this happens? You can optionally visualize the frequency of ground-truth answers in the training set.}
10 most frequent classes are: ['Other', 'yes', '2', 'no', 'baseball', 'broccoli', 'red', 'sheep', '3', 'surfing'] \\
Because network is not expressive enough to capture the causal factors the data, it will try to capture the output distribution. That is, because the output distribution, the distribution of the labels in the training dataset, is largely skewed, having a majority of "other", "yes", and so on as we can see in the histogram, the network will try to predict these top frequent classes as much as possible to get better accuracy.
\newpage

\setcounter{section}{3}
\section*{Task 3: Transformer Network}

\setcounter{subsection}{0}
\subsection{Complete the CrossAttentionLayer}
\begin{lstlisting}
class CrossAttentionLayer(nn.Module):
    """Self-/Cross-attention between two sequences."""

    def __init__(self, d_model=256, dropout=0.1, n_heads=8):
        """Initialize layers, d_model is the encoder dimension."""
        super().__init__()

        # Self-attention for seq1
        self.sa1 = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )  # use batch_first=True everywhere!
        self.dropout_1 = nn.Dropout(dropout)
        self.norm_1 = nn.LayerNorm(d_model)

        # Self-attention for seq2
        self.sa2 = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.dropout_2 = nn.Dropout(dropout)
        self.norm_2 = nn.LayerNorm(d_model)

        # Cross attention from seq1 to seq2
        self.cross_12 = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.dropout_12 = nn.Dropout(dropout)
        self.norm_12 = nn.LayerNorm(d_model)

        # FFN for seq1
        self.ffn_12 = nn.Sequential(
            nn.Linear(d_model, 1024),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(1024, d_model),
            nn.Dropout(dropout),
        )
        self.norm_122 = nn.LayerNorm(d_model)

        # Cross attention from seq2 to seq1
        self.cross_21 = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.dropout_21 = nn.Dropout()
        self.norm_21 = nn.LayerNorm(d_model)

        # FFN for seq2
        self.ffn_21 = nn.Sequential(
            nn.Linear(d_model, 1024),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(1024, d_model),
            nn.Dropout(dropout),
        )
        self.norm_212 = nn.LayerNorm(d_model)

    def forward(self, seq1, seq1_key_padding_mask, seq2,
                seq2_key_padding_mask,
                seq1_pos=None, seq2_pos=None):
        """Forward pass, seq1 (B, S1, F), seq2 (B, S2, F)."""
        # Self-attention for seq1
        q1 = k1 = v1 = seq1
        if seq1_pos is not None:
            q1 = q1 + seq1_pos
            k1 = k1 + seq1_pos
        seq1b = self.sa1(
            query=q1,
            key=k1,
            value=v1,
            attn_mask=None,
            key_padding_mask=seq1_key_padding_mask  # (B, S1)
        )[0]
        seq1 = self.norm_1(seq1 + self.dropout_1(seq1b))

        # Self-attention for seq2
        q2 = k2 = v2 = seq2
        if seq2_pos is not None:
            q2 = q2 + seq2_pos
            k2 = k2 + seq2_pos
        seq2b = self.sa2(
            query=q2,
            key=k2,
            value=v2,
            attn_mask=None,
            key_padding_mask=seq2_key_padding_mask  # (B, S2)
        )[0]
        seq2 = self.norm_2(seq2 + self.dropout_2(seq2b))

        # Create key, query, value for seq1, seq2
        q1 = k1 = v1 = seq1
        q2 = k2 = v2 = seq2
        if seq1_pos is not None:
            q1 = q1 + seq1_pos
            k1 = k1 + seq1_pos
        if seq2_pos is not None:
            q2 = q2 + seq2_pos
            k2 = k2 + seq2_pos

        # Cross-attention from seq1 to seq2 and FFN
        seq1b = self.cross_12(
            query=q1,
            key=k2,
            value=v2,
            attn_mask=None,
            key_padding_mask=seq2_key_padding_mask  # (B, S2)
        )[0]
        seq1 = self.norm_12(seq1 + self.dropout_12(seq1b))

        # FFN for seq1
        seq1 = self.norm_122(seq1 + self.ffn_12(seq1))

        # Cross-attention from seq2 to seq1 and FFN
        seq2b = self.cross_21(
            query=q2,
            key=k1,
            value=v1,
            attn_mask=None,
            key_padding_mask=seq1_key_padding_mask  # (B, S1)
        )[0]
        seq2 = self.norm_21(seq2 + self.dropout_21(seq2b))

        # FFN for seq2
        seq2 = self.norm_212(seq2 + self.ffn_21(seq2))

        return seq1, seq2
\end{lstlisting}

\subsection{Load the trained weights and reproduce our result.}
Final Val Acc: 0.6762130489169794

\subsection{How does the histogram of answers look now?}
\includegraphics[width=\textwidth]{q3.3.png}
The histogram of answers look much better now with a peak that is much lower, more different sub-peaks, and more less frequent answers (not appearing just once). The top answer is "yes", not "other" in previous task.
\newpage

\setcounter{section}{4}
\section*{Task 4: Bells and Whistles}

\setcounter{subsection}{0}
\subsection{Is it a matter of capacity? In the BaselineNet replace the linear classifier with a deeper MLP. Report the accuracy plot of this model.}
No it is not a matter of capacity. I replaced the linear classifier with two hidden layer MLP with hidden size of 2048.
\begin{lstlisting}
self.classifier = nn.Sequential(
    nn.Linear(
        self.text_encoder.config.hidden_size + 512,
        2048,
    ),
    nn.ReLU(),
    nn.Linear(
        2048,
        2048,
    ),
    nn.ReLU(),
    nn.Linear(
        2048,
        n_answers,
    )
)
\end{lstlisting}
\includegraphics[width=\textwidth]{q4.1_1.png}
\includegraphics[width=\textwidth]{q4.1_2.png}
\includegraphics[width=\textwidth]{q4.1_3.png}
\includegraphics[width=\textwidth]{q4.1_4.png}
The final val accuracy I got is 0.5249522680887484, which is even slightly worse than the baseline. The validation accuracy plot looks very unstable with jumping backwards among epochs. This is probably because the classifier is too large to train sufficiently. It shows that it is not a matter of capacity.

\end{document}